---
title: "Problem Set 3"
author: "SOC-GA 2332 Intro to Stats (Spring 2021)"
date: 'Due: Saturday, Apr. 17th, 11:59 pm'
output:
  pdf_document: default
  html_document:
    df_print: paged
---
```{r setup}
knitr::opts_chunk$set(echo = TRUE)
library(pacman)
p_load(tidyverse, kableExtra, sandwich, scales, stargazer)

```

# Part 1 Assumptions of OLS Regression

Recall that in our first lecture on regression, we talked about the **Gauss-Markov Assumptions**. If all these assumptions are met, the OLS estimator is the **Best Linear Unbiased Estimator** (**BLUE**). In a simple bivariate case, if the "true" data-generating process is $Y = \beta_0 + \beta_1 X + \epsilon$. The Gauss-Markov Assumptions can be state as the following:  

  (a) **Linearity**: A linear relationship between $X$ and $Y$ hold in the sample.  
  (b) **Exogeneity of Predictors**: The conditional mean of the error term, given the predictor, is zero ($\mathbf{x} = [x_1, x_2, ..., x_n]^\top$ is the value vector of $X$):
  $$E[ \epsilon_i | \mathbf{x} ] = 0 \text{,   for all   }i = 1,2,...,n.$$  
  (c) **No Perfect Collinearity**: Explanatory variables cannot be perfectly correlated.  
  (d) **Homoskedasticity**:  
      - No Heteroskedasticity: The conditional variance of the error term, given the predictor, is constant: $Var[ \epsilon_i | \mathbf{x} ] = \sigma^2 \text{,   for all   }i = 1,2,...,n.$  
      - No Autocorrelation: Conditional on the predictor, the error terms are uncorrelated across the observations: $Cov[\epsilon_i, \epsilon_j | \mathbf{x} ] = 0 \text{   ,      }i \ne j.$  
  
  
**1.** [15pts] For each of the assumptions, discuss what will go wrong when the assumption is violated. Be brief in your answers. *Note*: In addition to class materials, you can learn more about these assumptions in the [$\text{\underline{Wikipedia article}}$](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem) on the Gauss–Markov theorem, particularly the "Gauss–Markov theorem as stated in econometrics" section. (You can skip all the mathematical proofs and remarks.)  
  
  (a) **Linearity**: If the linearity assumption is violated, then an OLS model would not be appropriate for these data. A non-linear model (like quadratic) might be used instead. 
  (b) **Exogeneity of Predictors**: If this assumption is violated (meaning the error term is correlated with explanatory variables, i.e.$\epsilon =/= 0) then that means part of the error term is predictable and we will not get an accurate prediction of expected values. 
  (c) **No Perfect Collinearity**: If there is perfect or multi-collinearity between variables, it will be difficult to interpret our regression coefficients because the standard errors have been inflated. 
  (d) **Homoskedasticity**: If there is heteroskedasticity, our standard errors of the coefficients are not reliable. Thus, our confidence intervals for the regression coefficients would either be too narrow or too wide, and thus inaccurate. 
      
      
**2.** [5pts] Let $\beta_0 = -0.25$, $\beta_1 = 1.2$, $X \sim \Gamma(5, 4)$, and $\epsilon \sim Normal(0, 1)$. Here, $\Gamma(\alpha, \psi)$ denotes the Gamma distribution with shape parameter $\alpha$ and rate parameter $\psi$. (You can search how to use R to simulate from this distribution.)  
  Simulate a dataset of size $n = 3,000$ from this process in which all of the assumptions you've discussed above hold. Estimate a OLS model and plot regression diagnostics of this model.  
  
  
```{r }
set.seed(1234)
X <- rgamma(3000,5,4)
Y_predicted <- -0.25 + 1.2*X + rnorm(3000,0,1)

df <- data.frame(
  X,
  Y_predicted)
```

```{r}
# Creating the scatterplot
df %>% ggplot(aes(x = X, y = Y_predicted)) +
  geom_point(shape =1, alpha = 0.7) +
  geom_smooth(method = 'lm') +
  labs(title = "Simulation with beta0 = -0.25,  
       beta1 = 1.2, X=Gamma(5, 4), and episolon=Normal(0, 1)", x = "X", y = "Predicted Y")
```

**Bonus Question** [10pts]: From assumption (a), (b) and (d), choose one assumption and simulate a data that violates that assumption (all other assumptions should be satisfied). Create a plot which illustrates how the violation of the assumption affects the regression results. This can be a scatterplot with both the "true" and "false" OLS lines, a sampling distribution of the OLS estimator (comparing your estimate model results with actual simulations), or anything that shows how the violation leads us to false decisions if we assume the assumption is true. (The point is to demonstrate a contrast between the "true" and the "false", not just diagnostics of the "false".)  

When simulating data, you don't have to use the parameters set in the previous problem.  

*Hint*: You can search how to use `+ stat_function()` to plot a nonlinear line when plotting with `ggplot()`, or search how to use the base R functions such as `plot()` and `curve()`.

# Simulating a violation of homoskedasticity 
```{r}
##-----------------------------------------------
# Simulate IV
set.seed(1234)
X <- rpois(1000, lambda = 6)  

# Simulate error term  to violate homoskedasticity 
set.seed(1234)
error_violate <- rnorm(1000, 0, 8*X)

#Simulate error term to satisfy homoskedasticity
set.seed(1234)
error_satsify <-rnorm(1000, 0, 3) # Here, error term isn't dependent on X  

# Calculate DV for violation
Y <- -3 + 25*X + error_violate
# Calculate DV for satisfy
Y2 <--3 + 25*X + error_satsify

# Put variables into a dataframes
violate_df<- tibble(X = X, Y = Y)
satisfy_df <- tibble(X = X, Y = Y2)

# Plot Y against X, violate_df
violate_df %>%
  ggplot(aes(x = X, y = Y)) +
  geom_point(shape = 1, alpha = 0.7) +
  labs(title = "Scatterplot of Simulated Data with Heteroskedasticity")
       
# Plot Y against X, satisfy_df
satisfy_df %>%
  ggplot(aes(x = X, y = Y2)) +
  geom_point(shape = 1, alpha = 0.7) +
  labs(title = "Scatterplot of Simulated Data with Homoskedasticity")


m_violate <- lm(Y ~ X, violate_df)
m_satisfy <- lm(Y2 ~ X, satisfy_df)
stargazer(m_violate, m_satisfy, type = "text")
```

**As shown in the regression table, when there is heteroskedasticity our regression coefficient is underestimated compared to when there is not heteroskedasticity (24.032 vs. 24.991, respectively).**

# Part 2 Causality

A study on COVID-19 constructed a "COVID risk factor" score based on the COVID infection rate of a given area (defined by zip code).  

A researcher wants to estimate the effect of having a vaccination center in the area on that area's COVID risk factor score. She compiled a dataset that contains each area's COVID risk factor score and whether the area has a vaccination center. She then estimated the effect of having a vaccination center using the "naive estimator" we discussed in class.  

You noted that the quality of information residents have about COVID and the vaccine can be a confounding variable that affects both the area's infection rate and whether there is a vaccination center in the area. Assume that you are able to estimate the relationships this "informedness"  confounder (`info`) and the original "vaccination center" predictor (`vaccine`) have with the COVID risk factor score (`covid_risk`), which can be simulated using the following code (`n` is sample size):

```
set.seed(1234) # set the same seed to ensure identical results
e = rnorm(n, 0, 0.5)
covid_risk = rescale( 0 - 7*vaccine - 2*info + e, to = c(0, 100))

```

**1.** [5pts] Import the data `covid.csv`, according to the counterfactual framework, constructing a counterfactual "risk factor" in the dataframe. 

```{r}
# Importing data
covid_data <- read.csv("covid.csv")
```


```{r}
 # set the same seed to ensure identical results
n = nrow(covid_data)
set.seed(1234)
e = rnorm(n, 0, 0.5)
covid_risk = rescale( 0 - 7*covid_data$vaccine - 2*covid_data$info + e, to = c(0, 100))
                      
# --------- Create a Df of Simulated Data ---------
covid_data <- covid_data %>% mutate(
  "Risk Factor" = covid_risk)

head(covid_data, 5)
```


```{r}
# --------- Add counterfactual variables---------
covid_data <- covid_data %>%
  mutate(vaccine_counterf = ifelse(vaccine == 1, 0, 1),
         risk_counterf = 0 - 7*vaccine_counterf - 2*info + e)

head(covid_data)
```

**2.** [10pts] Fill out the table below (round to 1 decimal points):

Group                     |     $Y^T$            |   $Y^C$
--------------------------|----------------------|----------------------
Treatment Group ($D = 1$) | $E[Y^T|D = 1] = 96.6$    |   $E[Y^C|D = 1] = 103.4$
Control Group ($D = 0$)  |   $E[Y^T|D = 0] = 31.7$       |  $E[Y^C|D = 0] = 24.8$

  
```{r}
treat <- covid_data %>% filter(covid_data$vaccine == 1)
control <- covid_data %>% filter(covid_data$vaccine == 0) 
cf_treat <- covid_data %>% filter(covid_data$vaccine_counterf == 0)
cf_control <- covid_data %>% filter(covid_data$vaccine_counterf == 1)
```


```{r}
mean(treat$"Risk Factor")
mean(control$"Risk Factor")
mean(cf_treat$risk_counterf)
mean(cf_control$risk_counterf)
```
  
  
**3.** [15pts]Estimate the following:

(a) The Naive Estimator of ATE
```{r}
mean(treat$`Risk Factor`) - mean(control$`Risk Factor`)
```

  
(b) Treatment Effect on the Treated 
```{r}
mean(treat$`Risk Factor`) - mean(cf_treat$risk_counterf)
```


(c) Treatment Effect on the Control
```{r}
mean(control$`Risk Factor`) - mean(cf_control$risk_counterf)
```

  
(d) Selection Bias
```{r}
mean(cf_treat$risk_counterf) - mean(control$`Risk Factor`)
```

  

**4**. [15pts] Write a non-technical, short summary reporting your results in response to the above mentioned researcher who used the naive estimation. Imagine that you are explaining this to an audience who may not be familiar with the specific terminologies of the counterfactual framework (such as ATE or Treatment Effect on the Treated), but is interested in your substantive findings.

[Your Answer Here]  


# Part 3 Linear Probability Model and Logistic Regression 

`admin.csv` contains a dataset of graduate school admission results with the following variables:

Variable Name|  Variable Detail
------------|----------------------------
 `admit`| Admission Dummy (Admitted is 1) 
 `gre`  | GRE score
 `gpa`| GPA
 `rank`   |  Institution Tier (Tier 1 to 4)
 

**1**. [10pts] Import `admin.csv` to your R environment. Estimate (a) a linear probability model and (b) a logistic regression model to predict the probability of being admitted based on the applicant's GRE, GPA, and institution tier. Display the two modeling results in a table.

```{r}
# Importing data
admin_data <- read.csv("admin.csv")
head(admin_data)
```

(a) Estimating linear probability model and (b) Estimate logistic regression model, display both in table  
```{r}
lpm1 <- lm(admit ~ gre + gpa + rank, admin_data)
logit1 <- glm(admit ~ gre + gpa + rank, admin_data, family = binomial(link="logit"))

stargazer(lpm1, logit1, type = "text")
```

**2**. [10pts] In one or two paragraphs, summarize your modeling result for each model.

[Your Answer Here]  


**3**. [15pts] Plot the predicted probability of admission based on one's GPA percentile and institution rank (holding GRE at the mean) for the logistic regression model. For the purpose of this exercise, please set the value of `gpa` to range from 1 to 4. Make sure to add appropriate title and labels to your figure.

```{r }
#create df with key IVs, fix GRE at mean 
pred_IV <- tibble(gpa = rep(1:4), rank = rep(1:4)) %>% mutate(gre = mean(admin_data$gre, na.rm = T))

# use `predict` to predict the Y
# the model you are using, the df you use, setting CI
predict_admit <- predict(logit1, pred_IV, interval = "confidence", level = 0.95)

# bind the columns
pred_result <- cbind(pred_IV, predict_admit)

# Plot
pred_result %>% 
  ggplot(aes(x = gpa, y = predict_admit)) +
  geom_line(aes(color = rank)) +   # group linetype by gender
  labs(x = "IQ",
       y = "Predicted SAT Math Score") +
  ggtitle("Predicted SAT Math Score by IQ and Gender",
          subtitle = "(Modeled with interaction between IQ and gender)")
```

# Part 4 (Not Graded) Final Replication Project

At this point, you should complete most of the data cleaning and start replicating the descriptive tables and figure. You can submit an additional PDF file if you have made progress in replication Table A1a, Table A1b, and Figure 1.  


